{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nabzhb77JOYq",
        "outputId": "95535f7a-0975-46dc-f049-b29e572f6e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from PIL import Image\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Define dataset paths\n",
        "ORIGINAL_TRAIN_PATH = \"/content/drive/MyDrive/Cavity Dataset/train/\"\n",
        "ORIGINAL_TEST_PATH = \"/content/drive/MyDrive/Cavity Dataset/test/\"\n",
        "COPIED_TRAIN_PATH = \"dataset_col/train/\"\n",
        "COPIED_TEST_PATH = \"dataset_col/test/\"\n",
        "TARGET_COUNT = 400  # Target number of images per class\n",
        "\n",
        "# Function to check if an image is valid (not corrupted)\n",
        "def is_valid_image(img_path):\n",
        "    try:\n",
        "        img = cv2.imread(img_path)\n",
        "        return img is not None\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# Function to copy dataset while removing corrupted images\n",
        "def copy_dataset(src, dest):\n",
        "    if os.path.exists(dest):\n",
        "        shutil.rmtree(dest)  # Remove existing copy\n",
        "    os.makedirs(dest, exist_ok=True)\n",
        "\n",
        "    for class_folder in os.listdir(src):\n",
        "        src_class_path = os.path.join(src, class_folder)\n",
        "        dest_class_path = os.path.join(dest, class_folder)\n",
        "        os.makedirs(dest_class_path, exist_ok=True)\n",
        "\n",
        "        for filename in os.listdir(src_class_path):\n",
        "            img_path = os.path.join(src_class_path, filename)\n",
        "            if is_valid_image(img_path):\n",
        "                shutil.copy(img_path, dest_class_path)\n",
        "\n",
        "# Create a copy of the dataset, removing corrupted images\n",
        "copy_dataset(ORIGINAL_TRAIN_PATH, COPIED_TRAIN_PATH)\n",
        "copy_dataset(ORIGINAL_TEST_PATH, COPIED_TEST_PATH)\n",
        "\n",
        "# Image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load copied dataset\n",
        "train_dataset = ImageFolder(root=COPIED_TRAIN_PATH, transform=transform)\n",
        "\n",
        "# Load pre-trained ResNet50 as feature extractor\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.resnet50(pretrained=True)\n",
        "model = nn.Sequential(*list(model.children())[:-1])  # Remove final classification layer\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Function to extract features\n",
        "def extract_features(dataset, indices):\n",
        "    dataloader = DataLoader(Subset(dataset, indices), batch_size=32, shuffle=False)\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in tqdm(dataloader, desc=\"Extracting Features\"):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            features.extend(outputs.squeeze().cpu().numpy())\n",
        "    return np.array(features)\n",
        "\n",
        "# Perform Adaptive Sampling via Clustering for both classes\n",
        "def adaptive_oversampling(class_name):\n",
        "    class_path = os.path.join(COPIED_TRAIN_PATH, class_name)\n",
        "    images = os.listdir(class_path)\n",
        "    image_paths = [os.path.join(class_path, img) for img in images]\n",
        "\n",
        "    # Extract features for clustering\n",
        "    indices = [i for i, (path, label) in enumerate(train_dataset.samples) if label == train_dataset.class_to_idx[class_name]]\n",
        "    features = extract_features(train_dataset, indices)\n",
        "\n",
        "    # Perform K-Means clustering\n",
        "    n_clusters = 5  # You can change this\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(features)\n",
        "\n",
        "    # Count samples per cluster and find the smallest\n",
        "    cluster_counts = np.bincount(cluster_labels)\n",
        "    min_cluster = np.argmin(cluster_counts)\n",
        "\n",
        "    # Get paths of images in the minority cluster\n",
        "    minority_cluster_paths = [image_paths[i] for i in range(len(image_paths)) if cluster_labels[i] == min_cluster]\n",
        "\n",
        "    # Oversample minority cluster images first\n",
        "    extra_needed = TARGET_COUNT - len(images)\n",
        "    oversample_images = minority_cluster_paths * (extra_needed // len(minority_cluster_paths)) + random.choices(minority_cluster_paths, k=extra_needed % len(minority_cluster_paths))\n",
        "\n",
        "    for i, img_path in enumerate(oversample_images):\n",
        "        filename = os.path.basename(img_path)\n",
        "        new_filename = f\"oversampled_{i}_{filename}\"\n",
        "        shutil.copy(img_path, os.path.join(class_path, new_filename))\n",
        "\n",
        "# Apply adaptive oversampling to both classes separately\n",
        "for class_name in train_dataset.classes:\n",
        "    adaptive_oversampling(class_name)\n",
        "\n",
        "# Extract features and labels for SMOTE/ADASYN\n",
        "def get_features_and_labels(dataset):\n",
        "    indices = list(range(len(dataset)))\n",
        "    features = extract_features(dataset, indices)\n",
        "    labels = [label for _, label in dataset.samples]\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "features, labels = get_features_and_labels(train_dataset)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Apply SMOTE or ADASYN\n",
        "sampler = SMOTE(random_state=SEED)  # Change to ADASYN() if needed\n",
        "resampled_features, resampled_labels = sampler.fit_resample(features, encoded_labels)\n",
        "\n",
        "# Decode labels back to class names\n",
        "resampled_labels = label_encoder.inverse_transform(resampled_labels)\n",
        "\n",
        "# Function to generate synthetic images (Placeholder for actual reconstruction)\n",
        "def generate_synthetic_images(features, labels, dataset, target_path):\n",
        "    class_dirs = {cls: os.path.join(target_path, cls) for cls in dataset.classes}\n",
        "\n",
        "    for class_dir in class_dirs.values():\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "    for i, (feat, label) in enumerate(zip(features, labels)):\n",
        "        class_name = dataset.classes[label]\n",
        "        save_path = os.path.join(class_dirs[class_name], f\"synthetic_{i}.png\")\n",
        "\n",
        "        # Convert feature vector into an image (Placeholder: replace with proper inversion)\n",
        "        img = np.random.rand(224, 224, 3) * 255  # Dummy image, replace with actual reconstruction\n",
        "        img = Image.fromarray(img.astype(np.uint8))\n",
        "        img.save(save_path)\n",
        "\n",
        "# Generate synthetic images and save them\n",
        "generate_synthetic_images(resampled_features, resampled_labels, train_dataset, COPIED_TRAIN_PATH)\n",
        "\n",
        "# Final class counts\n",
        "final_counts = {cls: len(os.listdir(os.path.join(COPIED_TRAIN_PATH, cls))) for cls in train_dataset.classes}\n",
        "print(f\"Final balanced class counts: {final_counts}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeSo-cilJm8y",
        "outputId": "8298e322-7b53-4677-e727-9245f7dcd84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 94.1MB/s]\n",
            "Extracting Features: 100%|██████████| 13/13 [00:06<00:00,  2.00it/s]\n",
            "Extracting Features: 100%|██████████| 10/10 [00:04<00:00,  2.21it/s]\n",
            "Extracting Features: 100%|██████████| 23/23 [00:10<00:00,  2.21it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final balanced class counts: {'cavity': 788, 'no_cavity': 788}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Pre-trained Models with Correct Weights\n",
        "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "inception = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Remove final classification layers\n",
        "resnet18 = nn.Sequential(*list(resnet18.children())[:-1])  # ResNet outputs (batch, 512, 1, 1)\n",
        "inception.fc = nn.Identity()  # Remove Inception's classification layer\n",
        "\n",
        "# Move models to device\n",
        "resnet18.to(device).eval()\n",
        "inception.to(device).eval()\n",
        "\n",
        "# Define Image Transformations (InceptionNet requires 299x299 input size)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load Training and Test Data\n",
        "train_dataset = datasets.ImageFolder(root=\"dataset_col/train/\", transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=\"dataset_col/test/\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Function to extract features from both models\n",
        "def extract_features(loader):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(loader, desc=\"Extracting Features\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Extract features\n",
        "            resnet_features = resnet18(images).squeeze(-1).squeeze(-1)  # Shape: (batch, 512)\n",
        "            inception_features = inception(images)  # Shape: (batch, 2048)\n",
        "\n",
        "            # Flatten and concatenate features\n",
        "            resnet_features = resnet_features.cpu().numpy()\n",
        "            inception_features = inception_features.cpu().numpy()\n",
        "\n",
        "            combined_features = np.hstack((resnet_features, inception_features))\n",
        "\n",
        "            # Store features and labels\n",
        "            features.extend(combined_features)\n",
        "            labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Extract features for training and testing\n",
        "X_train, y_train = extract_features(train_loader)\n",
        "X_test, y_test = extract_features(test_loader)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Generate Classification Report\n",
        "report = classification_report(y_test, y_pred, target_names=train_dataset.classes)\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRj1_DuAJshC",
        "outputId": "0785018d-55b8-4c30-b3c7-0ab694d4a08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 112MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 110MB/s] \n",
            "Extracting Features: 100%|██████████| 50/50 [00:24<00:00,  2.06it/s]\n",
            "Extracting Features:  83%|████████▎ | 5/6 [00:02<00:00,  1.77it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. \n",
            "  warnings.warn(str(msg))\n",
            "Extracting Features: 100%|██████████| 6/6 [00:03<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      cavity       0.86      0.92      0.89        97\n",
            "   no_cavity       0.89      0.81      0.85        79\n",
            "\n",
            "    accuracy                           0.87       176\n",
            "   macro avg       0.87      0.86      0.87       176\n",
            "weighted avg       0.87      0.87      0.87       176\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Pre-trained Models\n",
        "resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "densenet121 = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Modify Models - Remove Final Classification Layers\n",
        "resnet50 = nn.Sequential(*list(resnet50.children())[:-1])  # Remove FC layer\n",
        "densenet121.classifier = nn.Identity()  # Remove classifier\n",
        "\n",
        "# Move models to device\n",
        "resnet50.to(device).eval()\n",
        "densenet121.to(device).eval()\n",
        "\n",
        "# Define Image Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Standard size for both models\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load Training and Test Data\n",
        "train_dataset = datasets.ImageFolder(root=\"dataset_col/train/\", transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=\"dataset_col/test/\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Function to Extract Features from Both Models\n",
        "def extract_features(loader):\n",
        "    features, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(loader, desc=\"Extracting Features\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Extract features\n",
        "            resnet_feats = resnet50(images).squeeze(-1).squeeze(-1).cpu().numpy()  # Shape: (batch, 2048)\n",
        "            densenet_feats = densenet121(images).cpu().numpy()  # Shape: (batch, 1024)\n",
        "\n",
        "            # Flatten and concatenate features\n",
        "            combined_features = np.hstack((resnet_feats, densenet_feats))\n",
        "\n",
        "            # Store features and labels\n",
        "            features.extend(combined_features)\n",
        "            labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Extract features for training and testing\n",
        "X_train, y_train = extract_features(train_loader)\n",
        "X_test, y_test = extract_features(test_loader)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Generate Classification Report\n",
        "report = classification_report(y_test, y_pred, target_names=train_dataset.classes)\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "terPGLG1LEho",
        "outputId": "3727a7f3-7f73-4329-e9e0-8fbc5ff4b94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 153MB/s]\n",
            "Extracting Features: 100%|██████████| 50/50 [00:20<00:00,  2.44it/s]\n",
            "Extracting Features:  83%|████████▎ | 5/6 [00:02<00:00,  1.72it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. \n",
            "  warnings.warn(str(msg))\n",
            "Extracting Features: 100%|██████████| 6/6 [00:03<00:00,  1.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      cavity       0.81      0.94      0.87        97\n",
            "   no_cavity       0.90      0.72      0.80        79\n",
            "\n",
            "    accuracy                           0.84       176\n",
            "   macro avg       0.86      0.83      0.83       176\n",
            "weighted avg       0.85      0.84      0.84       176\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Pre-trained Models\n",
        "inception = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
        "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Modify Models - Remove Final Classification Layers\n",
        "inception.fc = nn.Identity()  # Remove classification layer\n",
        "efficientnet.classifier = nn.Identity()  # Remove classification layer\n",
        "\n",
        "# Move models to device\n",
        "inception.to(device).eval()\n",
        "efficientnet.to(device).eval()\n",
        "\n",
        "# Define Image Transformations (Inception requires 299x299 input)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),  # Needed for InceptionV3\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load Training and Test Data\n",
        "train_dataset = datasets.ImageFolder(root=\"dataset_col/train/\", transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=\"dataset_col/test/\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Function to Extract Features from Both Models\n",
        "def extract_features(loader):\n",
        "    features, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(loader, desc=\"Extracting Features\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Extract features\n",
        "            inception_feats = inception(images).cpu().numpy()  # Shape: (batch, 2048)\n",
        "            efficientnet_feats = efficientnet(images).cpu().numpy()  # Shape: (batch, 1280)\n",
        "\n",
        "            # Flatten and concatenate features\n",
        "            combined_features = np.hstack((inception_feats, efficientnet_feats))\n",
        "\n",
        "            # Store features and labels\n",
        "            features.extend(combined_features)\n",
        "            labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Extract features for training and testing\n",
        "X_train, y_train = extract_features(train_loader)\n",
        "X_test, y_test = extract_features(test_loader)\n",
        "\n",
        "# Train Support Vector Machine (SVM)\n",
        "svm_classifier = SVC(kernel='rbf', C=1.0)  # Radial Basis Function kernel\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Generate Classification Report\n",
        "report = classification_report(y_test, y_pred, target_names=train_dataset.classes)\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Aw5shReLmkA",
        "outputId": "7bcc5ba2-ba1b-4bc6-e6d9-a0d117d5c05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 93.7MB/s]\n",
            "Extracting Features: 100%|██████████| 50/50 [00:25<00:00,  1.95it/s]\n",
            "Extracting Features:  83%|████████▎ | 5/6 [00:02<00:00,  1.70it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. \n",
            "  warnings.warn(str(msg))\n",
            "Extracting Features: 100%|██████████| 6/6 [00:03<00:00,  1.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      cavity       0.85      0.91      0.88        97\n",
            "   no_cavity       0.88      0.80      0.83        79\n",
            "\n",
            "    accuracy                           0.86       176\n",
            "   macro avg       0.86      0.85      0.86       176\n",
            "weighted avg       0.86      0.86      0.86       176\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Pre-trained Models\n",
        "mobilenet = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
        "densenet = models.densenet169(weights=models.DenseNet169_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Modify Models - Remove Final Classification Layers\n",
        "mobilenet.classifier = nn.Identity()  # Remove classification layer\n",
        "densenet.classifier = nn.Identity()  # Remove classification layer\n",
        "\n",
        "# Move models to device\n",
        "mobilenet.to(device).eval()\n",
        "densenet.to(device).eval()\n",
        "\n",
        "# Define Image Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Standard input size for both models\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load Training and Test Data\n",
        "train_dataset = datasets.ImageFolder(root=\"dataset_col/train/\", transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=\"dataset_col/test/\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Function to Extract Features from Both Models\n",
        "def extract_features(loader):\n",
        "    features, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(loader, desc=\"Extracting Features\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Extract features\n",
        "            mobilenet_feats = mobilenet(images).cpu().numpy()  # Shape: (batch, 1280)\n",
        "            densenet_feats = densenet(images).cpu().numpy()  # Shape: (batch, 1664)\n",
        "\n",
        "            # Flatten and concatenate features\n",
        "            combined_features = np.hstack((mobilenet_feats, densenet_feats))\n",
        "\n",
        "            # Store features and labels\n",
        "            features.extend(combined_features)\n",
        "            labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Extract features for training and testing\n",
        "X_train, y_train = extract_features(train_loader)\n",
        "X_test, y_test = extract_features(test_loader)\n",
        "\n",
        "# Train K-Nearest Neighbors (KNN) Classifier\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5, metric='euclidean')  # Using 5 neighbors with Euclidean distance\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = knn_classifier.predict(X_test)\n",
        "\n",
        "# Generate Classification Report\n",
        "report = classification_report(y_test, y_pred, target_names=train_dataset.classes)\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7inSu_AmMfMA",
        "outputId": "81cf2e6e-47e0-40a7-f93f-5eefbef80532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n",
            "100%|██████████| 21.1M/21.1M [00:00<00:00, 85.4MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to /root/.cache/torch/hub/checkpoints/densenet169-b2777c0a.pth\n",
            "100%|██████████| 54.7M/54.7M [00:00<00:00, 130MB/s]\n",
            "Extracting Features: 100%|██████████| 50/50 [00:20<00:00,  2.50it/s]\n",
            "Extracting Features:  83%|████████▎ | 5/6 [00:03<00:00,  1.46it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. \n",
            "  warnings.warn(str(msg))\n",
            "Extracting Features: 100%|██████████| 6/6 [00:03<00:00,  1.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      cavity       0.83      0.81      0.82        97\n",
            "   no_cavity       0.78      0.80      0.79        79\n",
            "\n",
            "    accuracy                           0.81       176\n",
            "   macro avg       0.80      0.81      0.81       176\n",
            "weighted avg       0.81      0.81      0.81       176\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Pre-trained Models\n",
        "vit = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Modify Models - Remove Final Classification Layers\n",
        "vit.heads = nn.Identity()  # Remove classification layer\n",
        "resnet18 = nn.Sequential(*list(resnet18.children())[:-1])  # Remove last classification layer\n",
        "\n",
        "# Move models to device\n",
        "vit.to(device).eval()\n",
        "resnet18.to(device).eval()\n",
        "\n",
        "# Define Image Transformations (ViT requires 224x224 input size)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Required for ViT and ResNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load Training and Test Data\n",
        "train_dataset = datasets.ImageFolder(root=\"dataset_col/train/\", transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=\"dataset_col/test/\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Function to Extract Features from Both Models\n",
        "def extract_features(loader):\n",
        "    features, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(loader, desc=\"Extracting Features\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Extract features\n",
        "            vit_feats = vit(images).cpu().numpy()  # Shape: (batch, 768)\n",
        "            resnet_feats = resnet18(images).squeeze(-1).squeeze(-1).cpu().numpy()  # Shape: (batch, 512)\n",
        "\n",
        "            # Flatten and concatenate features\n",
        "            combined_features = np.hstack((vit_feats, resnet_feats))\n",
        "\n",
        "            # Store features and labels\n",
        "            features.extend(combined_features)\n",
        "            labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Extract features for training and testing\n",
        "X_train, y_train = extract_features(train_loader)\n",
        "X_test, y_test = extract_features(test_loader)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_classifier = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, use_label_encoder=False, eval_metric=\"mlogloss\")\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Generate Classification Report\n",
        "report = classification_report(y_test, y_pred, target_names=train_dataset.classes)\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvVhQYKVM0C6",
        "outputId": "d82dcd7c-2b33-4f35-df71-981eed0c7d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:03<00:00, 115MB/s] \n",
            "Extracting Features: 100%|██████████| 50/50 [00:31<00:00,  1.57it/s]\n",
            "Extracting Features:  83%|████████▎ | 5/6 [00:03<00:00,  1.35it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. \n",
            "  warnings.warn(str(msg))\n",
            "Extracting Features: 100%|██████████| 6/6 [00:03<00:00,  1.51it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [10:43:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      cavity       0.81      0.88      0.84        97\n",
            "   no_cavity       0.83      0.75      0.79        79\n",
            "\n",
            "    accuracy                           0.82       176\n",
            "   macro avg       0.82      0.81      0.81       176\n",
            "weighted avg       0.82      0.82      0.82       176\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Pre-trained Models\n",
        "inception = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
        "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Modify Models - Remove Final Classification Layers\n",
        "inception.fc = nn.Identity()  # Remove classification layer\n",
        "resnet18 = nn.Sequential(*list(resnet18.children())[:-1])  # Remove last classification layer\n",
        "\n",
        "# Move models to device\n",
        "inception.to(device).eval()\n",
        "resnet18.to(device).eval()\n",
        "\n",
        "# Define Image Transformations (Inception requires 299x299 input size)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),  # Required for InceptionV3\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load Training and Test Data\n",
        "train_dataset = datasets.ImageFolder(root=\"dataset_col/train/\", transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=\"dataset_col/test/\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Function to Extract Features from Both Models\n",
        "def extract_features(loader):\n",
        "    features, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(loader, desc=\"Extracting Features\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Extract features\n",
        "            inception_feats = inception(images).cpu().numpy()  # Shape: (batch, 2048)\n",
        "            resnet_feats = resnet18(images).squeeze(-1).squeeze(-1).cpu().numpy()  # Shape: (batch, 512)\n",
        "\n",
        "            # Flatten and concatenate features\n",
        "            combined_features = np.hstack((inception_feats, resnet_feats))\n",
        "\n",
        "            # Store features and labels\n",
        "            features.extend(combined_features)\n",
        "            labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Extract features for training and testing\n",
        "X_train, y_train = extract_features(train_loader)\n",
        "X_test, y_test = extract_features(test_loader)\n",
        "\n",
        "# First Classifier: Random Forest\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "X_train_rf = rf_classifier.predict_proba(X_train)\n",
        "X_test_rf = rf_classifier.predict_proba(X_test)\n",
        "\n",
        "# Second Classifier: Support Vector Machine (SVM)\n",
        "svm_classifier = SVC(probability=True, kernel='rbf')\n",
        "svm_classifier.fit(X_train_rf, y_train)\n",
        "X_train_svm = svm_classifier.predict_proba(X_train_rf)\n",
        "X_test_svm = svm_classifier.predict_proba(X_test_rf)\n",
        "\n",
        "# Third Classifier: XGBoost\n",
        "xgb_classifier = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, use_label_encoder=False, eval_metric=\"mlogloss\")\n",
        "xgb_classifier.fit(X_train_svm, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = xgb_classifier.predict(X_test_svm)\n",
        "\n",
        "# Generate Classification Report\n",
        "report = classification_report(y_test, y_pred, target_names=train_dataset.classes)\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCxhQIPpNC0a",
        "outputId": "548ce71c-07b7-427d-914d-7f2c13017d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 50/50 [00:24<00:00,  2.08it/s]\n",
            "Extracting Features:  83%|████████▎ | 5/6 [00:02<00:00,  1.79it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. \n",
            "  warnings.warn(str(msg))\n",
            "Extracting Features: 100%|██████████| 6/6 [00:02<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      cavity       0.91      0.77      0.84        97\n",
            "   no_cavity       0.77      0.91      0.83        79\n",
            "\n",
            "    accuracy                           0.84       176\n",
            "   macro avg       0.84      0.84      0.84       176\n",
            "weighted avg       0.85      0.84      0.84       176\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [10:46:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    }
  ]
}